### Lecture: 3. Off-Policy Methods, N-step, Function Approximation, DQN
#### Date: Mar 4
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl139/2324/slides/?03
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl139/2324/slides.pdf/npfl139-2324-03.pdf, PDF Slides
#### Questions: #lecture_3_questions

- Off-policy Monte Carlo Methods [Sections 5.5-5.7 of RLB]
- Expected Sarsa [Section 6.6 of RLB]
- N-step TD policy evaluation [Section 7.1 of RLB]
- N-step Sarsa [Section 7.2 of RLB]
- Off-policy n-step Sarsa [Section 7.3 of RLB]
- Tree backup algorithm [Section 7.5 of RLB]
- Function approximation [Sections 9-9.3 of RLB]
- Tile coding [Section 9.5.4 of RLB]
- Linear function approximation [Section 9.4 of RLB, without the Proof of Convergence if Linear TD(0)]
- Semi-Gradient TD methods [Sections 9.3, 10-10.2 of RLB]
- Off-policy function approximation TD divergence [Sections 11.2-11.3 of RLB]
- Deep Q Network [[Volodymyr Mnih et al.: Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)]
