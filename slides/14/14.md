title: NPFL139, Lecture 14
class: title, langtech, cc-by-sa
# MARL, External Memory,<br>RLHF, DPO

## Milan Straka

### May 20, 2024

---
section: MARL
# Partially Observable MDPs

![w=51%,f=right](../01/pomdp.svgz)

Recall that a **partially observable Markov decision process** extends the
Markov decision process to a sextuple $(ùì¢, ùìê, p, Œ≥, ùìû, o)$, where the MDP
components
- $ùì¢$ is a set of states,
- $ùìê$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a **reward** $r ‚àà ‚Ñù$,
- $Œ≥ ‚àà [0, 1]$ is a **discount factor**,

are extended by:
- $ùìû$ is a set of observations,
- $o(O_{t+1} | S_{t+1}, A_t)$ is an observation model, where observation $O_t$
  is used as agent input instead of the state $S_t$.

---
# Partially Observable Stochastic Game

![w=30%,f=right](marl_posg.svgz)

A **partially observable stochastic game (POSG)** is a 9-tuple
$(ùì¢, N, \{ùìê^{i‚àà[N]}\}, \{Œ©^{i‚àà[N]}\}, \{R^{i‚àà[N]}\}, P, \{O^{i‚àà[N]}\}, œÅ_0, Œ≥)$, where

- $ùì¢$ is the set of all possible _states_,
~~~
- $N$ is the _number of agents_,
~~~
- $ùìê^{i}$ is the set of all possible _actions_ for agent
  $i$, with $ùìê^Œ† ‚âù ‚àè_i ùìê^i$,
~~~
- $Œ©^{i}$ is the set of all possible _observations_ for agent $i$,
~~~
- $R^{i}(r^i_{t+1} ‚àà ‚Ñù | s_t ‚àà ùì¢, ‚Üía_t ‚àà ùìê^Œ†, s_{t+1} ‚àà ùì¢)$ is the _reward function_ for agent $i$,
~~~
- $O^{i}(œâ^i_{t+1} ‚àà Œ©^i | s_{t+1} ‚àà ùì¢, a^i_t ‚àà ùìê)$ is the _observation model_ for agent $i$,
  a distribution of observing $w^i_{t+1}$ after performing action $a^i_t$
  leading to state $s_{t+1}$,
~~~
- $P(s_{t+1} ‚àà ùì¢ | s_t ‚àà ùì¢, ‚Üía_t ‚àà ùìê^Œ†)$ is the _transition model_,
~~~
- $œÅ_{0}$ is the _initial state distribution_,
~~~
- $Œ≥ ‚àà [0, 1]$ is a _discount factor_.

---
# Partially Observable Stochastic Game

![w=30%,f=right](marl_posg.svgz)

We denote
- joint actions/policy/observation across all agents as vectors
  $$‚Üía ‚âù (a^1, ‚Ä¶, a^N) ‚àà ùìê^Œ†,$$
~~~
- joint actions/policy/observation for all agents but agent $i$ as
  $$‚Üía^{-i} ‚âù (a^1, ‚Ä¶, a^{i-1}, a^{i+1}, ‚Ä¶, a^N),$$

---
# Agent-Environment Cycle Game

However, when actually implementing POSG, various ambiguities exist in the
order of execution. Therefore, **agent-environment cycle game (AECG)** has been
proposed,  
a 12-tuple $(ùì¢, N, \{ùìê^{i‚àà[N]}\}, \{Œ©^{i‚àà[N]}\}, \{R^{i‚àà[N]}\}, \{T^{i‚àà[N]}\}, P, \{O^{i‚àà[N]}\}, V, s_0, i_0, Œ≥)$ where

- $ùì¢$ is the set of all possible _states_,
~~~
- $N$ is the _number of agents_, including $0$ for ‚Äúenvironment‚Äù agent; $[N^‚à™] ‚âù [N] \cup \{0\}$,
~~~
- $ùìê^{i}$ is the set of all possible _actions_ for agent $i$, with $ùìê^0 ‚âù \{‚àÖ\}$, $ùìê^‚à™ ‚âù \bigcup_{i‚àà[N^‚à™]} ùìê^i$,
~~~
- $Œ©^{i}$ is the set of all possible _observations_ for agent $i$,
~~~
- $R^{i}(r^i_{t+1} ‚àà ‚Ñù | s_t ‚àà ùì¢, j ‚àà [N^‚à™], a^j_t ‚àà ùìê^j, s_{t+1} ‚àà ùì¢)$ is the _reward distribution_ for agent $i$,
~~~
- $T^{i} : ùì¢ √ó ùìê^i ‚Üí ùì¢$ is the deterministic _transition function_ for agent $i$,
~~~
- $P(s_{t+1} ‚àà ùì¢ | s_t ‚àà ùì¢)$ is the transition function for the environment,
~~~
- $O^{i}(œâ^i_{t+1} ‚àà Œ©^i | s_{t+1} ‚àà ùì¢)$ is the _observation model_ for agent $i$,
~~~
- $V(j ‚àà [N^‚à™] | s_t ‚àà ùì¢, i ‚àà [N^‚à™], a^i_t ‚àà ùìê^i)$ is the _next agent function_,
~~~
- $s_{0} ‚àà ùì¢$ is the _initial state_,
~~~
- $i_{0} ‚àà [N^U]$ is the _initial agent_,
~~~
$\,‚àô\,\,\,Œ≥ ‚àà [0, 1]$ is a _discount factor_.

---
# Agent-Environment Cycle Game

![w=42%](marl_posg.svgz)![w=58%](marl_aecg.svgz)

It holds that for every POSG, there is an equivalent AECG, and vice versa.

---
# Game Settings

Depending on the reward function, there are several game settings:

~~~
- **fully cooperative**, when $‚àÄi, ‚àÄj: R^i(s_t, ‚Üía_t, s_{t+1}) = R^j(s_t, ‚Üía_t, s_{t+1})$,

~~~
- **cooperative**, when $‚àÄi, ‚àÄj, ‚àÉk>0 : R^i(s_t, ‚Üía_t, s_{t+1}) ‚â• k R^j(s_t, ‚Üía_t, s_{t+1})$,
~~~
- **competitive**, when $‚àÉi, ‚àÉj, ‚àÉk<0 : R^i(s_t, ‚Üía_t, s_{t+1}) ‚â• k R^j(s_t, ‚Üía_t, s_{t+1})$,
~~~
- **zero-sum**, when $‚àë_{i‚àà[N]} R^i(s_t, ‚Üía_t, s_{t+1}) = 0$,

---
# The MARL Problem

We define a trajectory $‚ÜíœÑ$ as a sequence of states and actions
$$‚ÜíœÑ ‚âù (s_0, ‚Üía_0, s_1, ‚Üía_1, s_2, ‚Ä¶),$$
where:
~~~
- $s_0 ‚àº œÅ_0$,
~~~
- $‚Üía_t ‚àº ‚ÜíœÄ(‚ãÖ|s_t)$,
~~~
- $s_{t+1} ‚Üí P(‚ãÖ|s_t, ‚Üía_t)$.

~~~
A return for an agent $i$ and trajectory $‚ÜíœÑ$ is
$$R^i(‚ÜíœÑ) ‚âù ‚àë_{t=0}^{|‚ÜíœÑ|} Œ≥^t r^i_{t+1}.$$

---
# The MARL Problem

For a given policy $‚ÜíœÄ$, the expected return for agent $i$ is
$$J^i(‚ÜíœÄ) ‚âù ùîº_{‚ÜíœÑ ‚àº ‚ÜíœÄ} \big[R^i(‚ÜíœÑ)\big],$$
~~~
where a probability of a trajetory $‚ÜíœÑ$ is
$$P(‚ÜíœÑ | ‚ÜíœÄ) ‚âù œÅ_0(s_0) ‚àè_{t=0}^{|‚ÜíœÑ|} P(s_{t+1} | s_t, ‚Üía_t) ‚ÜíœÄ(‚Üía_t | s_t).$$ 

~~~
For a given joing policy $‚ÜíœÄ^{-i}$, **best response** is
$$œÄÃÇ^i(‚ÜíœÄ^{-i}) ‚âù \argmax_{œÄ_i} J^i(œÄ^i, ‚ÜíœÄ^{-i})$$

---
style: .halftable { display: inline-block; width: 49% } .halftable table {margin: auto}
# The MARL Goal

It is unfortunately not clear what the goal of MARL should be, given that it is
a multi-criterion optimization problem.

~~~
For zero-sum games, we can consider **Nash equilibrium**, which is a joint
policy $‚ÜíœÄ_*$ fulfilling
$$‚àÄ i ‚àà[N], ‚àÄ_œÄ^i: J^i(‚ÜíœÄ_*) ‚â• J^i(œÄ^i, ‚ÜíœÄ^{-i}).$$

~~~
In other words, $œÄ^i_*$ is a best response to $‚ÜíœÄ^{-i}_*$ for all agents $i$.

~~~
Under reasonable assumptions, a Nash equilibrium exists.
~~~
Unfortunately, there can be multiple Nash equilibria with different payoffs
(Nash equilibrium is just a ‚Äúlocal‚Äù optimum).

<div class="halftable">

- Stag hunt

| | Stag | Rabbit |
|:---|:--:|:--:|
| Stag | 2\2 | 0\1 |
| Rabbit | 1\0 | 1\1 |

</div><div class="halftable">

- Prisoner's dilemma

| | Stag | Rabbit |
|:---|:--:|:--:|
| Stag | 2\2 | 0\1 |
| Rabbit | 1\0 | 1\1 |

</div>

---
# MARL Training Schemes

## Centralized Scheme

![w=50%,h=center](marl_scheme_centralized.svgz)

A joint model for all agents, a single critic.

---
# MARL Training Schemes

## Concurrent/Parameter-Sharing Scheme

![w=80%,h=center,mw=50%](marl_scheme_concurrent.svgz)![w=90%,h=center,mw=50%](marl_scheme_parameter_sharing.svgz)

Each agent is trained independently. When the agents are homogenous, their
models can be optionally shared (the _parameter-sharing scheme_).

However, the environment is then non-stationary, and using a replay buffer is
problematic because of changing policies of other agents.

---
# MARL Training Schemes

## Centralized Training with Decentralized Execution

![w=50%,h=center](marl_scheme_ctde.svgz)

Quite a common model, where the agents are independent, but the critics get
the observations and actions of all agents.

---
# Multi-Agent Deep Deterministic Policy Gradient

![w=100%,v=middle](maddpg_scheme.svgz)

---
section: MARL Algorithms
# Multi-Agent Deep Deterministic Policy Gradient

![w=71%,f=left,h=center,mw=65%](maddpg_algorithm.svgz)

~~~
Alternatively, in multi-agent settings, in some experiments it was beneficial
to estimate the gradient for the policy update using the current policy instead
of the action from the replay buffer; if the line 14 is changed to
$$‚àá^i_{‚ÜíŒ∏} \frac{1}{|B|} ‚àë_{‚Üíœâ} Q^i_œÜ\big(‚Üíœâ, ‚ÜíŒº_{‚ÜíŒ∏}(‚Üíœâ)\big),$$
we talk about _Soft MADDPG_.

---
# Multi-Agent Twin Delayed DDPG

![w=100%,v=middle](matd3_scheme.svgz)

---
# Multi-Agent Twin Delayed DDPG

![w=60%,f=left,h=center,mw=65%](matd3_algorithm.svgz)

~~~
We can again consider a _Soft MATD3_ variant.

~~~
Furthermore, we can also use the minimum of both critics during
policy update (shown to be beneficial by DDPG++ and SAC). The resulting
algorithm is called _(Soft) MATD4_.

---
section: MARL Experiments
# MARL Evaluation, Simple Target

![w=60%](marl_simple_target.svgz)![w=40%](marl_simple_collect.svgz)

Reward is given for touching a landmark, and for unoccupied landmarks
also for disatance of the nearest agent (orignally any agent, but easier
variant is an agent not occupying a landmark).

~~~
The agents have non-negligible size and get negative reward for colliding.

~~~
Actions can be discrete (‚àÖ, ‚Üê, ‚Üí, ‚Üë, ‚Üì; ST Gumbel-softmax is used) or continuous.

~~~
In the _Simple Collect_ variant, the targets disappear after being occupied for
some time, and a new one appears on a random location.

---
# MARL Evaluation, Simple Target, Continuous Actions

![w=71%,h=center](marl_simple_target_continuous_graph.svgz)

---
# MARL Evaluation, Simple Target, Continuous Actions

![w=59%,h=center](marl_simple_target_continuous_table.svgz)

---
# MARL Evaluation, Simple Target, Discrete Actions

![w=71%,h=center](marl_simple_target_discrete_graph.svgz)

---
# MARL Evaluation, Simple Target, Discrete Actions

![w=59%,h=center](marl_simple_target_discrete_table.svgz)

---
# MARL Evaluation, Simple Confuse

![w=50%,h=center](marl_simple_confuse.svgz)

Some number of cooperaing agents gets rewarded based on the minimum distance of
any agent to the target landmark; but are penalized based on the distance
of a single adversary to the target landmark.

~~~
The adversary gets rewarded based on its distance to the target landmark;
however, it does not know which landmark is the target one.

~~~
Actions can be again either discrete or continuous.

---
# MARL Evaluation, Simple Confuse, Continuous Actions

![w=71%,h=center](marl_simple_confuse_continuous_graph.svgz)

---
# MARL Evaluation, Simple Confuse, Continuous Actions

![w=59%,h=center](marl_simple_confuse_continuous_table.svgz)

---
# MARL Evaluation, Simple Confuse, Discrete Actions

![w=71%,h=center](marl_simple_confuse_discrete_graph.svgz)

---
# MARL Evaluation, Simple Confuse, Discrete Actions

![w=59%,h=center](marl_simple_confuse_discrete_table.svgz)

---
section: HideAndSeek
# Multi-Agent Hide-and-Seek

As another example, consider https://openai.com/blog/emergent-tool-use/.

---
section: MERLIN
# MERLIN

In a partially-observable environment, keeping all information in the RNN state
is substantially limiting. Therefore, _memory-augmented_ networks can be used to
store suitable information in external memory (in the lines of NTM, DNC, or MANN
models).

We now describe an approach used by Merlin architecture (_Unsupervised
Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).

![w=95.75%,mw=50%,h=center](merlin_rl-lstm.svgz)![w=95%,mw=50%,h=center](merlin_rl-mem.svgz)

---
# MERLIN ‚Äì Memory Module

![w=30%,f=right](merlin_rl-mem.svgz)

Let $‚ÜíM$ be a memory matrix of size $N_\textit{mem} √ó 2|‚Üíe|$.

~~~
Assume we have already encoded observations as $‚Üíe_t$ and previous action
$a_{t-1}$. We concatenate them with $K$ previously read vectors and process
them by a deep LSTM (two layers are used in the paper) to compute $‚Üíh_t$.

~~~
Then, we apply a linear layer to $‚Üíh_t$, computing $K$ key vectors
$‚Üík_1, ‚Ä¶, ‚Üík_K$ of length $2|‚Üíe|$ and $K$ positive scalars $Œ≤_1, ‚Ä¶, Œ≤_K$.

~~~
**Reading:** For each $i$, we compute cosine similarity of $‚Üík_i$ and all memory
rows $‚ÜíM_j$, multiply the similarities by $Œ≤_i$ and pass them through a $\softmax$
to obtain weights $‚Üíœâ_i$. The read vector is then computed as $‚áâM ‚Üíœâ_i$.

~~~
**Writing:** We find one-hot write index $‚Üív_\textit{wr}$ to be the least used
memory row (we keep usage indicators and add read weights to them). We then
compute $‚Üív_\textit{ret} ‚Üê Œ≥ ‚Üív_\textit{ret} + (1 - Œ≥) ‚Üív_\textit{wr}$,
and retroactively update the memory matrix using
$‚áâM ‚Üê ‚áâM + ‚Üív_\textit{wr}[‚Üíe_t, 0] + ‚Üív_\textit{ret}[0, ‚Üíe_t]$.

---
# MERLIN ‚Äî Prior and Posterior

However, updating the encoder and memory content purely using RL is inefficient.
Therefore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.
The goal of MBP is to compress observations into low-dimensional state
representations $‚Üíz$ and storing them in memory.

~~~
We want the state variables not only to faithfully represent the data, but also
emphasise rewarding elements of the environment above irrelevant ones. To
accomplish this, the authors follow the hippocampal representation theory of
Gluck and Myers, who proposed that hippocampal representations pass through
a compressive bottleneck and then reconstruct input stimuli together with task
reward.

~~~
In MERLIN, a (Gaussian diagonal) _prior_ distribution over $‚Üíz_t$ predicts next state variable
conditioned on history of state variables and actions
$p(‚Üíz_t^\textrm{prior} | ‚Üíz_{t-1}, a_{t-1}, ‚Ä¶, ‚Üíz_1, a_1)$,
and _posterior_ corrects the prior using the new observation $‚Üío_t$, forming
a better estimate $q(‚Üíz_t | ‚Üío_t, ‚Üíz_t^\textrm{prior}, ‚Üíz_{t-1}, a_{t-1}, ‚Ä¶, ‚Üíz_1, a_1) + ‚Üíz_t^\textrm{prior}$.

---
# MERLIN ‚Äî Prior and Posterior

To achieve the mentioned goals, we add two terms to the loss.

- We try reconstructing input stimuli, action, reward and return using a sample from
  the state variable posterior, and add the difference of the reconstruction and
  ground truth to the loss.

~~~
- We also add KL divergence of the prior and the posterior to the loss, to ensure
  consistency between the prior and the posterior.

~~~
![w=85%,h=center](merlin_diagram.svgz)

---
# MERLIN ‚Äî Algorithm

![w=100%](merlin_algorithm.svgz)

---
# MERLIN

![w=70%,h=center](merlin_tasks.svgz)

---
# MERLIN

![w=50%,h=center](merlin_analysis.svgz)

---
# MERLIN

![w=90%,h=center](merlin_predictive_model.svgz)

---
section: CTF-FTW
# For the Win agent for Capture The Flag

![w=100%](ctf_overview.svgz)

---
# For the Win agent for Capture The Flag

- Extension of the MERLIN architecture.

~~~
- Hierarchical RNN with two timescales.

~~~
- V-Trace with both clipping factors set to 1 is used.

~~~
- Rewards for 13 pre-defined events (picking a flag, returning a flag, tagging/being tag with/without a flag, ‚Ä¶) are learned by the agent.

~~~
- Population based training controlling KL divergence penalty weights,
  internal dense rewards, slow ticking RNN speed, and gradient flow factor from
  fast to slow RNN.

  In every game, teams of similarly skilled agents were selected, and the
  authors state it is crucial to employ several agents instead of just one
  (30 simultaneously trained agents are used).

---
# For the Win agent for Capture The Flag

![w=47%,h=center](ctf_architecture.svgz)

---
# For the Win agent for Capture The Flag

![w=80%,h=center](ctf_curves.svgz)

