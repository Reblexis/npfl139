title: NPFL139, Lecture 3
class: title, langtech, cc-by-sa
# Function Approximation, DQN

## Milan Straka

### March 11, 2024

---
section: Semi-GradTD
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again bootstrap the estimate $v_π(S_t)$ as
$R_{t+1} + [¬\textrm{done}]⋅γv̂(S_{t+1}; →w)$.

~~~
![w=70%,h=center](grad_td_estimation.svgz)

---
# Why Semi-Gradient TD

Note that the above algorithm is called **semi-gradient**, because it does not
backpropagate through $v̂(S_{t+1}; →w)$:
$$→w ← →w + α\big(R_{t+1} + [¬\textrm{done}]⋅γv̂(S_{t+1}; →w) - v̂(S_t; →w)\big) ∇_{→w} v̂(S_t; →w).$$

~~~
In other words, the above rule is in fact not an SGD update, because there does
not exist a function $J(→w)$, for which we would get the above update.

~~~
To sketch a proof, consider a linear $v̂(S_t; →w) = ∑_i x(S_t)_i w_i$ and assume such a $J(→w)$ exists.
Then
$$\tfrac{∂}{∂w_i}J(→w) = \big(R_{t+1} + γv̂(S_{t+1}; →w) - v̂(S_t; →w)\big) x(S_t)_i.$$

~~~
Now considering second derivatives, we see they are not equal, which is a contradiction:
$$\begin{aligned}
  \tfrac{∂}{∂w_i}\tfrac{∂}{∂w_j}J(→w) &= \big(γx(S_{t+1})_i - x(S_t)_i\big) x(S_t)_j = γx(S_{t+1})_i x(S_t)_j - x(S_t)_i x(S_t)_j \\
  \tfrac{∂}{∂w_j}\tfrac{∂}{∂w_i}J(→w) &= \big(γx(S_{t+1})_j - x(S_t)_j\big) x(S_t)_i = γx(S_{t+1})_j x(S_t)_i - x(S_t)_i x(S_t)_j
\end{aligned}$$

---
# Temporal Difference Semi-Gradient Convergence

It can be proven (by using separate theory than for SGD) that the linear
semi-gradient TD methods do converge.

~~~
However, they do not converge to the optimum of $\overline{VE}$. Instead, they
converge to a different **TD fixed point** $→w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(→w_\mathrm{TD}) ≤ \frac{1}{1-γ} \min_{→w} \overline{VE}(→w).$$

~~~
However, when $γ$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=65%,h=center](grad_td_nstep_estimation.svgz)

---
# Temporal Difference Semi-Gradient Policy Evaluation

On the left, the results of one-step TD(0) algorithm is presented.
The effect of increasing $n$ in an $n$-step variant is displayed on the right.

![w=100%](grad_td_estimation_example.svgz)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.svgz)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.svgz)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.png)

The performances are for semi-gradient Sarsa($λ$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.svgz)
![w=50%,h=center](mountain_car_performance_nstep.svgz)

---
section: Off-policyDiver
# Off-policy Divergence With Function Approximation

Consider a deterministic transition between two states whose values are computed
using the same weight:

![w=20%,h=center](off_policy_divergence_idea.svgz)

~~~
- If initially $w=10$, the TD error will be also 10 (or nearly 10 if $γ<1$).
~~~
- If for example $α=0.1$, $w$ will be increased to 11 (by 10%).
~~~
- This process can continue indefinitely.

~~~
However, the problem arises only in off-policy setting, where we do not decrease
value of the second state from further observation.

---
# Off-policy Divergence With Function Approximation

The previous idea can be implemented for instance by the following **Baird's
counterexample**:

![w=77%,h=center](off_policy_divergence_example.svgz)

The rewards are zero everywhere, so the value function is also zero everywhere.
We assume the initial values of weights are 1, except for $w_7=10$, and that the
learning rate $α=0.01$.

---
# Off-policy Divergence With Function Approximation

For off-policy semi-gradient Sarsa, or even for off-policy
dynamic-programming update (where we compute expectation over all following
states and actions), the weights diverge to $+∞$.
Using on-policy distribution converges fine.

$$→w ← →w + \frac{α}{|𝓢|} ∑_s \Big(𝔼_π \big[R_{t+1} + γv̂(S_{t+1}; →w) | S_t=s\big] - v̂(s; →w)\Big) ∇v̂(s; →w)$$

![w=47%](off_policy_divergence_example.svgz)![w=53%](off_policy_divergence_results.svgz)

---
# Off-policy Divergence With Function Approximation

The divergence can happen when all following elements are combined:

- functional approximation;

~~~
- bootstrapping;

~~~
- off-policy training.

In the Sutton's and Barto's book, these are called **the deadly triad**.

---
section: DQN
# Deep Q Networks

Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv),

~~~
in Feb 2015 accepted in Nature as _Human-level control through deep reinforcement learning_.

~~~
Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

~~~
Training can be extremely brittle (and can even diverge as shown earlier).

---
# Deep Q Network

![w=85%,h=center](dqn_architecture.svgz)

---
# Deep Q Networks

- Preprocessing: $210×160$ 128-color images are converted to grayscale and
  then resized to $84×84$.
~~~
- Frame skipping technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- Input to the network are last $4$ frames (considering only the frames kept by
  frame skipping), i.e., an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8×8$ with stride 4 and ReLU,
  - 64 filters of size $4×4$ with stride 2 and ReLU,
  - 64 filters of size $3×3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$𝓛 ≝ 𝔼_{(s, a, r, s')∼\mathrm{data}}\left[(r + \left[¬\textrm{done}\right] ⋅ γ \max\nolimits_{a'} Q(s', a'; →θ̄) - Q(s, a; →θ))^2\right].$$
~~~
- An $ε$-greedy behavior policy is utilized (starts at $ε=1$ and gradually decreases to $0.1$).

Important improvements:
~~~
- **experience replay**: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly
  (off-policy training);
~~~
- separate **target network** $→θ̄$: to prevent instabilities, a separate _target
  network_ is used to estimate one-step returns. The weights are not trained,
  but copied from the trained network after a fixed number of gradient updates;
~~~
- reward clipping: because rewards have wildly different scale in different
  games, all positive rewards are replaced by $+1$ and negative by $-1$;
  life loss is used as an end of episode.
~~~
  - furthermore, $(r + \left[¬\textrm{done}\right] ⋅ γ \max_{a'} Q(s', a'; →θ̄) - Q(s, a; →θ))$ is
    also clipped to $[-1, 1]$ (i.e., a $\textrm{smooth}_{L_1}$ loss or Huber loss).

---
# Deep Q Networks

![w=60%,h=center](dqn_algorithm.svgz)

---
# Deep Q Network

![w=40%,h=center](dqn_results.svgz)

---
# Deep Q Network

![w=80%,h=center](dqn_visualization_breakout.svgz)

---
# Deep Q Network

![w=100%,v=middle](dqn_visualization_pong.svgz)


---
class: tablewide
style: td:nth-of-type(1) {width: 75%}
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
~~~
| replay buffer size | 1M |
~~~
| target network update frequency | 10k |
~~~
| discount factor | 0.99 |
~~~
| training frames | 50M |
~~~
| RMSProp learning rate and both momentums | 0.00025, 0.95 |
~~~
| initial $ε$, final $ε$ (linear decay) and frame of final $ε$ | 1.0, 0.1, 1M |
~~~
| replay start size | 50k |
~~~
| no-op max | 30 |
