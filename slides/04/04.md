title: NPFL139, Lecture 4
class: title, langtech, cc-by-sa
# Function Approximation,<br>Deep Q Network, Rainbow

## Milan Straka

### March 11, 2024

---
# Function Approximation

We will approximate value function $v$ and/or action-value function $q$,
selecting it from a family of functions parametrized by a weight vector $→w ∈ ℝ^d$.

We denote the approximations as
$$\begin{gathered}
  v̂(s; →w),\\
  q̂(s, a; →w).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(→w) ≝ ∑_{s∈𝓢} μ(s) \left[v_π(s) - v̂(s, →w)\right]^2,$$
where the state distribution $μ(s)$ is usually on-policy distribution.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $→w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  →w_{t+1} &← →w_t - \tfrac{1}{2} α ∇_{→w_t} \big(v_π(S_t) - v̂(S_t; →w_t)\big)^2\\
           &← →w_t + α\big(v_π(S_t) - v̂(S_t; →w_t)\big) ∇_{→w_t} v̂(S_t; →w_t).\\
\end{aligned}$$

As usual, the $v_π(S_t)$ is estimated by a suitable sample of a return:
- in Monte Carlo methods, we use episodic return $G_t$,
- in temporal difference methods, we employ bootstrapping and use
  one-step return
  $$R_{t+1} + [¬\textrm{done}]⋅γv̂(S_{t+1}; →w)$$
  or an $n$-step return.

---
section: Semi-Gradient
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again bootstrap the estimate $v_π(S_t)$ as
$R_{t+1} + [¬\textrm{done}]⋅γv̂(S_{t+1}; →w)$.

~~~
![w=70%,h=center](grad_td_estimation.svgz)

---
# Why Semi-Gradient TD

Note that the above algorithm is called **semi-gradient**, because it does not
backpropagate through $v̂(S_{t+1}; →w)$:
$$→w ← →w + α\big(R_{t+1} + [¬\textrm{done}]⋅γv̂(S_{t+1}; →w) - v̂(S_t; →w)\big) ∇_{→w} v̂(S_t; →w).$$

~~~
In other words, the above rule is in fact not an SGD update, because there does
not exist a function $J(→w)$, for which we would get the above update.

~~~
To sketch a proof, consider a linear $v̂(S_t; →w) = ∑_i x(S_t)_i w_i$ and assume such a $J(→w)$ exists.
Then
$$\tfrac{∂}{∂w_i}J(→w) = \big(R_{t+1} + γv̂(S_{t+1}; →w) - v̂(S_t; →w)\big) x(S_t)_i.$$

~~~
Now considering second derivatives, we see they are not equal, which is a contradiction:
$$\begin{aligned}
  \tfrac{∂}{∂w_i}\tfrac{∂}{∂w_j}J(→w) &= \big(γx(S_{t+1})_i - x(S_t)_i\big) x(S_t)_j = γx(S_{t+1})_i x(S_t)_j - x(S_t)_i x(S_t)_j \\
  \tfrac{∂}{∂w_j}\tfrac{∂}{∂w_i}J(→w) &= \big(γx(S_{t+1})_j - x(S_t)_j\big) x(S_t)_i = γx(S_{t+1})_j x(S_t)_i - x(S_t)_i x(S_t)_j
\end{aligned}$$

---
# Temporal Difference Semi-Gradient Convergence

It can be proven (by using separate theory than for SGD) that the linear
semi-gradient TD methods do converge.

~~~
However, they do not converge to the optimum of $\overline{VE}$. Instead, they
converge to a different **TD fixed point** $→w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(→w_\mathrm{TD}) ≤ \frac{1}{1-γ} \min_{→w} \overline{VE}(→w).$$

~~~
However, when $γ$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=65%,h=center](grad_td_nstep_estimation.svgz)

---
# Temporal Difference Semi-Gradient Policy Evaluation

On the left, the results of one-step TD(0) algorithm are presented.
The effect of increasing $n$ in an $n$-step variant is displayed on the right.

![w=100%](grad_td_estimation_example.svgz)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.svgz)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.svgz)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.png)

The performances are for semi-gradient Sarsa($λ$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.svgz)
![w=50%,h=center](mountain_car_performance_nstep.svgz)

---
section: Off-policy Divergence
# Off-policy Divergence With Function Approximation

Consider a deterministic transition between two states whose values are computed
using the same weight:

![w=20%,h=center](off_policy_divergence_idea.svgz)

~~~
- If initially $w=10$, the TD error will be also 10 (or nearly 10 if $γ<1$).
~~~
- If for example $α=0.1$, $w$ will be increased to 11 (by 10%).
~~~
- This process can continue indefinitely.

~~~
However, the problem arises only in off-policy setting, where we do not decrease
value of the second state from further observation.

---
# Off-policy Divergence With Function Approximation

The previous idea can be implemented for instance by the following **Baird's
counterexample**:

![w=77%,h=center](off_policy_divergence_example.svgz)

The rewards are zero everywhere, so the value function is also zero everywhere.
We assume the initial values of weights are 1, except for $w_7=10$, and that the
learning rate $α=0.01$.

---
# Off-policy Divergence With Function Approximation

For off-policy semi-gradient Sarsa, or even for off-policy
dynamic-programming update (where we compute expectation over all following
states and actions), the weights diverge to $+∞$.
Using on-policy distribution converges fine.

$$→w ← →w + \frac{α}{|𝓢|} ∑_s \Big(𝔼_π \big[R_{t+1} + γv̂(S_{t+1}; →w) | S_t=s\big] - v̂(s; →w)\Big) ∇v̂(s; →w)$$

![w=47%](off_policy_divergence_example.svgz)![w=53%](off_policy_divergence_results.svgz)

---
# Off-policy Divergence With Function Approximation

The divergence can happen when all following elements are combined:

- functional approximation;

~~~
- bootstrapping;

~~~
- off-policy training.

In the Sutton's and Barto's book, these are called **the deadly triad**.

---
section: DQN
# Deep Q Networks

Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv),

~~~
in Feb 2015 accepted in Nature as _Human-level control through deep reinforcement learning_.

~~~
Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

~~~
Training can be extremely brittle (and can even diverge as shown earlier).

---
# Deep Q Network

![w=85%,h=center](dqn_architecture.svgz)

---
# Deep Q Networks

- Preprocessing: $210×160$ 128-color images are converted to grayscale and
  then resized to $84×84$.
~~~
- **Frame skipping** technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- **Frame stacking** is utilizied – the input to the network are the last $4$
  frames (considering only the frames kept by frame skipping), i.e., the network
  inpus is an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8×8$ with stride 4 and ReLU,
  - 64 filters of size $4×4$ with stride 2 and ReLU,
  - 64 filters of size $3×3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$𝓛 ≝ 𝔼_{(s, a, r, s')∼\mathrm{data}}\left[(r + \left[¬\textrm{done}\right] ⋅ γ \max\nolimits_{a'} Q(s', a'; →θ̄) - Q(s, a; →θ))^2\right].$$
~~~
- An $ε$-greedy behavior policy is utilized (starts at $ε=1$ and gradually decreases to $0.1$).

Important improvements:
~~~
- **experience replay**: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly
  (off-policy training);
~~~
- separate **target network** $→θ̄$: to prevent instabilities, a separate _target
  network_ is used to estimate one-step returns. The weights are not trained,
  but copied from the trained network after a fixed number of gradient updates;
~~~
- reward clipping: because rewards have wildly different scale in different
  games, all positive rewards are replaced by $+1$ and negative by $-1$;
  life loss is used as an end of episode.
~~~
  - furthermore, $(r + \left[¬\textrm{done}\right] ⋅ γ \max_{a'} Q(s', a'; →θ̄) - Q(s, a; →θ))$ is
    also clipped to $[-1, 1]$ (i.e., a $\textrm{smooth}_{L_1}$ loss or Huber loss).

---
# Deep Q Networks

![w=60%,h=center](dqn_algorithm.svgz)

---
# Deep Q Network

![w=40%,h=center](dqn_results.svgz)

---
# Deep Q Network

![w=80%,h=center](dqn_visualization_breakout.svgz)

---
# Deep Q Network

![w=100%,v=middle](dqn_visualization_pong.svgz)


---
class: tablewide
style: td:nth-of-type(1) {width: 75%}
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
~~~
| replay buffer size | 1M |
~~~
| target network update frequency | 10k |
~~~
| discount factor | 0.99 |
~~~
| training frames | 50M |
~~~
| RMSProp learning rate and both momentums | 0.00025, 0.95 |
~~~
| initial $ε$, final $ε$ (linear decay) and frame of final $ε$ | 1.0, 0.1, 1M |
~~~
| replay start size | 50k |
~~~
| no-op max | 30 |

---
section: Rainbow
# Rainbow

There have been many suggested improvements to the DQN architecture. In the end
of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_
paper combines 6 of them into a single architecture they call **Rainbow**.

~~~
![w=38%,h=center](rainbow_results.svgz)

---
section: DDQN
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $ε$-greedy variant of the target
policy, the same samples (up to $ε$-greedy) determine both the maximizing action
and estimate its value.

~~~
![w=75%,h=center](../03/double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](../03/double_q_learning.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Similarly to double Q-learning, instead of
$$r + γ \max_{a'} Q(s', a'; →θ̄) - Q(s, a; →θ),$$
we minimize
$$r + γ Q(s', \argmax_{a'}Q(s', a'; →θ); →θ̄) - Q(s, a; →θ).$$

~~~
![w=30%,h=center](ddqn_errors.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=100%,h=center](ddqn_errors_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=60%,h=center](ddqn_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Performance on episodes taking at most 5 minutes and no-op starts on 49 games:
![w=40%,h=center,mh=40%,v=middle](ddqn_results_5min.svgz)

~~~
Performance on episodes taking at most 30 minutes and using human starts on 49
games:
![w=55%,h=center,mh=40%,v=middle](ddqn_results_30min.svgz)

---
section: Prioritized Replay
# Rainbow DQN Extensions

## Prioritized Replay

Instead of sampling the transitions uniformly from the replay buffer,
we instead prefer those with a large TD error. Therefore, we sample transitions
according to their probability
$$p_t ∝ \Big|r + γ \max_{a'} Q(s', a'; →θ̄) - Q(s, a; →θ)\Big|^ω,$$
~~~
where $ω$ controls the shape of the distribution (which is uniform for $ω=0$
and corresponds to TD error for $ω=1$).

~~~
New transitions are inserted into the replay buffer with maximum probability
to support exploration of all encountered transitions.

~~~
When combined with DDQN, the probabilities are naturally computed as
$$p_t ∝ \Big|r + γ Q(s', \argmax_{a'}Q(s', a'; →θ); →θ̄) - Q(s, a; →θ)\Big|^ω,$$

---
# Rainbow DQN Extensions

## Prioritized Replay

Because we now sample transitions according to $p_t$ instead of uniformly,
on-policy distribution and sampling distribution differ. To compensate, we
therefore utilize importance sampling with ratio
$$ρ_t = \left( \frac{1/N}{p_t} \right) ^β.$$

~~~
The authors utilize in fact “for stability reasons”
$$ρ_t / \max_i ρ_i.$$

---
# Rainbow DQN Extensions

## Prioritized Replay

![w=75%,h=center](prioritized_dqn_algorithm.svgz)

---
section: Dueling Networks
# Rainbow DQN Extensions

## Dueling Networks

Instead of computing directly $Q(s, a; →θ)$, we compose it from the following quantities:
~~~
- average return in a given state $s$, $V(s; →θ) = \frac{1}{|𝓐|} ∑_a Q(s, a; →θ)$,
~~~
- advantage function computing an **advantage** $Q(s, a; →θ) - V(s; θ)$ of action $a$ in state $s$.
~~~

![w=25%,h=center](dqn_dueling_architecture.png)

~~~
$$Q(s, a) ≝ V\big(f(s; ζ); η\big) + A\big(f(s; ζ), a; ψ\big) - \frac{\sum_{a' ∈ 𝓐} A(f(s; ζ), a'; ψ)}{|𝓐|}$$

---
# Rainbow DQN Extensions

## Dueling Networks

![w=100%,h=center](dqn_dueling_corridor.svgz)

---
# Rainbow DQN Extensions

## Dueling Networks

![w=32%,h=center](dqn_dueling_visualization.svgz)

---
# Rainbow DQN Extensions

## Dueling Networks

Results on all 57 games (retraining the original DQN on the 8 missing games).
`Single` refers to DDQN with a direct computation of $Q(s, a; →θ)$, `Clip`
corresponds to additional gradient clipping to norm at most 10 and larger
first hidden layer (so that duelling and single have roughly the same
number of parameters).

![w=70%,h=center,mh=65%,v=middle](dqn_dueling_results.svgz)
