title: NPFL139, Lecture 10
class: title, langtech, cc-by-sa
# R2D2, Agent57, PPO

## Milan Straka

### April 22, 2024

---
section: TransRews
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $𝓣$
$$(𝓣q)(s, a) ≝ 𝔼_{s',r ∼ p} \Big[r + γ \max_{a'} q(s', a')\Big].$$

~~~
Instead of clipping the magnitude of rewards, we might use a function
$h: ℝ → ℝ$ to reduce their scale. We define a transformed Bellman operator
$𝓣_h$ as
$$(𝓣_hq)(s, a) ≝ 𝔼_{s',r ∼ p} \Big[h\Big(r + γ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = α z$ for $α > 0$, then $𝓣_h^k q \xrightarrow{k → ∞} h \circ q_* = α q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $α$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $𝓣_h^k q \xrightarrow{k → ∞} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ 𝓣 q_* = h \circ 𝓣(h^{-1} \circ h \circ q_*) = 𝓣_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

For stochastic MDP, the authors prove that if $h$ is strictly monotonically
increasing, Lipschitz continuous with Lipschitz constant $L_h$, and has a
Lipschitz continuous inverse with Lipschitz constant $L_{h^{-1}}$, then
for $γ < \frac{1}{L_h L_{h^{-1}}}$, $𝓣_h$ is again a contraction. (Proof
in Proposition A.1.)

~~~
For the Atari environments, the authors propose the transformation
$$h(x) ≝ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + εx$$
with $ε = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4ε (|x| + 1 + ε)} - 1}{2ε} \right)^2 - 1\right).$$

~~~
In practice, discount factor larger than $\frac{1}{L_h L_{h^{-1}}}$ is being
used – however, it seems to work.

---
section: R2D2
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches by
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Rewards are transformed instead of clipped, and no loss-of-life-as-episode-end
heuristic is used.

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

~~~
The prioritized replay employs a combination of the maximum and the average
absolute 5-step TD errors $δ_i$ over the sequence: $p = η \max_i δ_i + (1 - η)
δ̄$, for both $η$ and the priority exponent set to 0.9.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=75%,h=center](r2d2_recurrent_staleness.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $γ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%,v=middle](r2d2_memory_size.svgz)

---
section: Agent57
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
# Retrace

$\displaystyle \mathrlap{𝓡q(s, a) ≝ q(s, a) + 𝔼_b \bigg[∑_{t≥0} γ^t \left(∏\nolimits_{j=1}^t c_t\right)
  \Big(R_{t+1} + γ𝔼_{A_{t+1} ∼ π} q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\Big)\bigg],}$

where there are several possibilities for defining the traces $c_t$:
~~~
- **importance sampling**, $c_t = ρ_t = \frac{π(A_t|S_t)}{b(A_t|S_t)}$,
  - the usual off-policy correction, but with possibly very high variance,
  - note that $c_t = 1$ in the on-policy case;
~~~
- **Tree-backup TB(λ)**, $c_t = λ π(A_t|S_t)$,
  - the Tree-backup algorithm extended with traces,
  - however, $c_t$ can be much smaller than 1 in the on-policy case;
~~~
- **Retrace(λ)**, $c_t = λ \min\big(1, \frac{π(A_t|S_t)}{b(A_t|S_t)}\big)$,
  - off-policy correction with limited variance, with $c_t = 1$ in the on-policy case.

~~~
The authors prove that $𝓡$ has a unique fixed point $q_π$ for any
$0 ≤ c_t ≤ \frac{π(A_t|S_t)}{b(A_t|S_t)}$.

---
# Never Give Up

The NGU (Never Give Up) agent performs _curiosity-driver exploration_, and
augment the extrinsic (MDP) rewards with an intrinsic reward. The augmented
reward at time $t$ is then $r_t^β ≝ r_t^e + β r_t^i$, with $β$ a scalar
weight of the intrinsic reward.

~~~
The intrinsic reward fulfills three goals:

~~~
1. quickly discourage visits of the same state in the same episode;

~~~
2. slowly discourage visits of the states visited many times in all episodes;

~~~
3. ignore the parts of the state not influenced by the agent's actions.

~~~
The intrinsic rewards is a combination of the episodic novelty $r_t^\textrm{episodic}$
and life-long novelty $α_t$:
$$r_t^i ≝ r_t^\textrm{episodic} ⋅ \operatorname{clip}\Big(1 ≤ α_t ≤ L=5\Big).$$

---
style: .katex-display { margin: .5em 0 }
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The episodic novelty works by storing the embedded states $f(S_t)$ visited
during the episode in episodic memory $M$.

~~~
The $r_t^\textrm{episodic}$ is then estimated as

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{\textrm{visit~count~of~}f(S_t)}}.$$

~~~
The visit count is estimated using similarities of $k$-nearest neighbors of $f(S_t)$
measured via an inverse kernel $K(x, z) = \frac{ε}{\frac{d(x, z)^2}{d_m^2} + ε}$ for
$d_m$ a running mean of the $k$-nearest neighbor distance:

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{∑\nolimits_{f_i ∈ N_k} K(f(S_t), f_i)}+c}\textrm{,~~with~pseudo-count~c=0.001}.$$

---
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The state embeddings are trained to ignore the parts not influenced by the actions of the agent.

~~~

To this end, Siamese network $f$ is trained to predict $p(A_t|S_t, S_{t+1})$,
i.e., the action $A_t$ taken by the agent in state $S_t$ when the resulting
state is $S_{t+1}$.

~~~
The life-long novelty $α_t=1 + \tfrac{\|ĝ - g\|^2 - μ_\textrm{err}}{σ_\textrm{err}}$
is trained using random network distillation (RND),
where a predictor network $ĝ$ tries to predict the output of an untrained
convolutional network $g$ by minimizing the mean squared error; the
$μ_\textrm{err}$ and $σ_\textrm{err}$ are the running mean and standard
deviation of the error $\|ĝ-g\|^2$.

---
# Never Give Up

![w=18%,f=right](ngu_architecture.svgz)

The NGU agent uses transformed Retrace loss with the augmented reward
$$r_t^i ≝ r_t^\textrm{episodic} ⋅ \operatorname{clip}\Big(1 ≤ α_t ≤ L=5\Big).$$

~~~
![w=23%,f=left](ngu_betas_gammas.svgz)

To support multiple policies concentrating either on the extrinsic or the
intrinsic reward, the NGU agent trains a parametrized action-value function $q(s, a, β_i)$
which corresponds to reward $r_t^{β_i}$ for $β_0=0$ and $γ_0=0.997$, …, $β_{N-1}=β$
and $γ_{N-1}=0.99$.

For evaluation, $q(s, a, 0)$ is employed.

---
# Never Give Up

![w=73%,h=center](ngu_results_table.svgz)
![w=75%,h=center](ngu_results.svgz)

---
# Never Give Up Ablations

![w=73%,h=center](ngu_ablations_embeddings.svgz)
![w=64%,h=center](ngu_ablations.svgz)

---
# Agent57

![w=32%,f=right](agent57_architecture.png)

Then Agent57 improves NGU with:
~~~
- splitting the action-value as $q(s, a, j; →θ) ≝ q(s, a, j; →θ^e) + β_j q(s, a, j; →θ^i)$, where

  - $q(s, a, j; →θ^e)$ is trained with $r_e$ as targets, and
  - $q(s, a, j; →θ^i)$ is trained with $r_i$ as targets.

~~~
- instead of considering all $(β_j, γ_j)$ equal, we train a meta-controller
  using a non-stationary multi-arm bandit algorithm, where arms correspond
  to the choice of $j$ for a whole episode (so an actor first samples a $j$
  using multi-arm bandit problem and then updates it according to the observed
  return), and the reward signal is the undiscounted extrinsic episode return;
  each actor uses a different level of $ε_l$-greedy behavior;

~~~
- $γ_{N-1}$ is increased from $0.997$ to $0.9999$.

---
# Agent57 – Results

![w=35%,h=center](agent57_results.svgz)
![w=89%,h=center](agent57_results_table.svgz)

---
# Agent57 – Ablations

![w=56%](agent57_ablations.svgz)![w=44%](agent57_ablations_arm.svgz)
